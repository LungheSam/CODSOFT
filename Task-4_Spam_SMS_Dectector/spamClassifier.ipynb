{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8x0RsEbYvZnG"},"outputs":[],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgEcmuqmveeh"},"outputs":[],"source":["import pandas as pd\n","\n","# Define the path to the dataset file in your Google Drive\n","file_path = \"/content/drive/MyDrive/ColabNotebooks/CODSOFT/SPAM_SMS_DETECTION/spam.csv\"\n","\n","\n","\n","# Load the dataset into a pandas DataFrame\n","dataset = pd.read_csv(file_path,encoding='latin1')\n","\n","# Display the first few rows of the dataset\n","dataset.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pNsNpEExPZG"},"outputs":[],"source":["dataset.describe()\n","display(dataset.columns,dataset.shape)\n","#dataset.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4kxrRCw0Hdx"},"outputs":[],"source":["dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RGnYv8fyA57"},"outputs":[],"source":["import pandas as pd\n","\n","def check_missing_values(df):\n","    \"\"\"\n","    Function to check for missing values in a dataset.\n","\n","    Parameters:\n","    - dataset: pandas DataFrame object representing the dataset.\n","\n","    Returns:\n","    - missing_values: pandas Series containing the count of missing values for each column.\n","    \"\"\"\n","    # Count the missing values for each column\n","    missing_values = dataset.isnull().sum()\n","    print(missing_values)\n","\n","check_missing_values(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2bl-W700uY4"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def plot_missing_value_percentage(df):\n","    \"\"\"\n","    Function to print the percentage of missing values for each column in the dataset\n","    and show a bar plot to visualize the missing value percentages.\n","\n","    Parameters:\n","    - dataset: pandas DataFrame object representing the dataset.\n","    \"\"\"\n","\n","    # Calculate the percentage of missing values for each column\n","    missing_percentage = (df.isnull().sum() / len(df)) * 100\n","\n","    # Plot the missing value percentages\n","    plt.figure(figsize=(10, 6))\n","    missing_percentage.plot(kind='bar', color='red')\n","    plt.title('Percentage of Missing Values in Each Column')\n","    plt.xlabel('Columns')\n","    plt.ylabel('Percentage of Missing Values')\n","    plt.xticks(rotation=0, ha='right')\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Print column-wise missing value percentages\n","    print(\"Column-wise missing value percentages:\")\n","    for column, percentage in missing_percentage.items():\n","        print(f\"{column}: {percentage:.2f}%\")\n","\n","plot_missing_value_percentage(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qw-2wgmX1cCg"},"outputs":[],"source":["import pandas as pd\n","\n","def drop_columns_with_high_missing_percentage(df, threshold=90):\n","    \"\"\"\n","    Function to drop columns from the dataset if the percentage of missing values\n","    in those columns is greater than the specified threshold.\n","\n","    Parameters:\n","    - dataset: pandas DataFrame object representing the dataset.\n","    - threshold: Percentage threshold for missing values (default is 90%).\n","\n","    Returns:\n","    - dataset_after_dropping: pandas DataFrame object with columns dropped.\n","    \"\"\"\n","\n","    # Calculate the percentage of missing values for each column\n","    missing_percentage = (df.isnull().sum() / len(df)) * 100\n","\n","    # Identify columns with missing percentage greater than the threshold\n","    columns_to_drop = missing_percentage[missing_percentage > threshold].index\n","\n","    # Drop the identified columns from the dataset\n","    dataset_after_dropping = df.drop(columns=columns_to_drop)\n","\n","    # Print columns dropped\n","    print(\"Columns dropped due to high missing percentage (> {}%):\".format(threshold))\n","    print(columns_to_drop)\n","\n","    return dataset_after_dropping\n","\n","\n","# Drop columns with high missing percentage (> 90%) from the dataset\n","dataset = drop_columns_with_high_missing_percentage(dataset)\n","dataset.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPw6dM8T2bjg"},"outputs":[],"source":["def print_column_value_counts(df):\n","  print(\"Printing Value Counts\")\n","  i=1\n","  for column in list(df.columns):\n","      print(f\"Column {i}:{column}\")\n","      print(\"-\"*30)\n","      print(f\"{df[column].value_counts()}\")\n","      print(\"-\"*30)\n","      i+=1\n","print_column_value_counts(dataset)\n","#dataset[\"v2\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsVhFmMuJGej"},"outputs":[],"source":["# def remove_duplicates(df):\n","#     # Print duplicates\n","#     duplicate_rows = df[df.duplicated()]\n","#     if not duplicate_rows.empty:\n","#         print(\"Duplicate Rows:\")\n","#         print(duplicate_rows)\n","#     else:\n","#         print(\"No duplicate rows found.\")\n","\n","#     # Remove duplicates\n","#     df.drop_duplicates(inplace=True)\n","\n","#     print(\"Duplicates removed.\")\n","#     return df\n","# dataset = remove_duplicates(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tViBMGGz2mfO"},"outputs":[],"source":["def encode_target_variable(df, target_column):\n","    # Create a copy of the dataset\n","    encoded_dataset = df.copy()\n","\n","    # Encode the target variable\n","    encoded_dataset[target_column] = encoded_dataset[target_column].map({\"ham\": 0, \"spam\": 1})\n","\n","    return encoded_dataset\n","\n","# Encode the target variable\n","dataset = encode_target_variable(dataset, target_column=\"v1\")\n","dataset.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffpXWOI23ipU"},"outputs":[],"source":[" dataset.rename(columns={\"v1\": \"target\",\"v2\":\"messages\"}, inplace=True)\n"," dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lvYMer3Phim"},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ORCZuklLgSM"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_correlation(df):\n","    # Calculate correlation matrix\n","    corr_matrix = df.corr()\n","\n","    # Plot correlation matrix\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","    plt.title('Correlation Matrix')\n","    plt.show()\n","\n","# Example usage:\n","# Assuming 'dataset' is the DataFrame containing your dataset\n","# You can call the function like this:\n","# plot_correlation(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YBPfwDk-J7F"},"outputs":[],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXBSnBysRFxB"},"outputs":[],"source":["import pandas as pd\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from collections import Counter\n","\n","def preprocessor(df):\n","    # Lowercasing\n","    df['cleaned_message'] = df['messages'].apply(lambda x: x.lower())\n","    # Punctuation Removal\n","    df['cleaned_message'] = df['cleaned_message'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n","    # Whitespace Normalization\n","    df['cleaned_message'] = df['cleaned_message'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n","    # URL Removal\n","    df['cleaned_message'] = df['cleaned_message'].apply(lambda x: re.sub(r'http\\S+', '', x))\n","    # Tokenization and Stopword Removal\n","    stop_words = set(stopwords.words('english'))\n","    df['tokens'] = df['cleaned_message'].apply(lambda x: word_tokenize(x))\n","    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","    # Stemming\n","    stemmer = PorterStemmer()\n","    df['tokens'] = df['tokens'].apply(lambda x:[stemmer.stem(token) for token in x])\n","    # Rare Word Removal\n","    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n","    token_counts = Counter(all_tokens)\n","    rare_words = set(token for token, count in token_counts.items() if count <= 1)\n","    df['processed_tokens'] = df['tokens'].apply(lambda x: ' '.join([token for token in x if token not in rare_words])) # Join tokens into a single string\n","    return df\n","# Apply preprocessing to the dataset\n","dataset = preprocessor(dataset)\n","\n","# Display the preprocessed dataset\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2jy-D7DUJST"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHjImXLT-s4B"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","\n","# Split the dataset into features (X) and target variable (y)\n","# X = dataset['messages']\n","# X = dataset['cleaned_message']\n","X = dataset['processed_tokens']\n","y = dataset['target']\n","\n","# Initialize TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Transform the text data into numerical features\n","X_tfidf = tfidf_vectorizer.fit_transform(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYFQ5_Fx9SYt"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","from sklearn.metrics import f1_score,accuracy_score,precision_score\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n","\n","# Initialize classifiers\n","classifiers = {\n","    \"Naive Bayes\": MultinomialNB(),\n","    \"Logistic Regression\": LogisticRegression(),\n","    \"Support Vector Machine\": SVC(),\n","    \"XGBoost Classifier\": XGBClassifier()\n","}\n","f1_scores=[]\n","accuracy_scores=[]\n","precision_scores=[]\n","# Train classifiers and calculate F1 score\n","for name, clf in classifiers.items():\n","    clf.fit(X_train, y_train)  # Assuming X_train_tfidf is the TF-IDF transformed training data\n","    y_pred = clf.predict(X_test)  # Assuming X_test_tfidf is the TF-IDF transformed testing data\n","    f1 = f1_score(y_test, y_pred, average='binary')\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","\n","    f1_scores.append((name,f1))\n","    accuracy_scores.append((name,accuracy))\n","    precision_scores.append((name,precision))\n","    print(f\"F1 Score for {name}: {f1}\")\n","    print(f\"Accuracy Score for {name}: {accuracy}\")\n","    print(f\"Precision Score for {name}: {precision}\")\n","    print(\"-\"*50)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_meZ2rm2AZqn"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_f1_scores(f1_Scores):\n","    \"\"\"\n","    Function to plot F1 scores of algorithms in a bar plot.\n","\n","    Parameters:\n","    - f1_scores: List of tuples containing algorithm names and corresponding F1 scores.\n","    \"\"\"\n","\n","    # Extract algorithm names and F1 scores from the list of tuples\n","    algorithms, scores = zip(*f1_Scores)\n","\n","    # Plot bar plot\n","    plt.figure(figsize=(10, 6))\n","    plt.barh(algorithms, scores, color='blue')\n","    plt.title('F1 Scores of Algorithms')\n","    plt.xlabel('F1 Score')\n","    plt.ylabel('Algorithms')\n","    plt.xlim(0, 1)  # Setting x-axis limit from 0 to 1\n","    plt.grid(axis='x', linestyle='--', alpha=0.7)\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_f1_scores(f1_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouwH9RkHjapf"},"outputs":[],"source":["print(f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6q8Rx7Kph5OM"},"outputs":[],"source":["algorithm_performance = pd.DataFrame(accuracy_scores, columns=['Algorithm', 'Accuracy'])\n","algorithm_performance['Precision'] = pd.DataFrame(precision_scores)[1]\n","algorithm_performance['F1 Score'] = pd.DataFrame(f1_scores)[1]\n","algorithm_performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GAAG5577_1e"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","\n","# Define a pipeline with TfidfVectorizer and MultinomialNB\n","pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer()),\n","    ('clf', MultinomialNB())\n","])\n","\n","# Define the parameter grid\n","param_grid = {\n","    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Unigrams or bigrams\n","    'tfidf__max_df': [0.5, 0.75, 1.0],  # Maximum document frequency\n","    'clf__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter\n","}\n","\n","# Create GridSearchCV object\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n","\n","# Train the model using GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters found\n","print(\"Best parameters:\", grid_search.best_params_)\n","\n","# Get the best model\n","best_model = grid_search.best_estimator_\n","\n","# Evaluate the best model\n","accuracy = best_model.score(X_test, y_test)\n","print(\"Accuracy on test set:\", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Syh-QS3-n48y"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","X = dataset['messages']\n","y = dataset['target']\n","\n","# Initialize TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","# Transform the text data into numerical features\n","X_tfidf = tfidf_vectorizer.fit_transform(X)\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n","xgb_classifier = XGBClassifier()\n","xgb_classifier.fit(X_train, y_train)  # Assuming X_train_tfidf is the TF-IDF transformed training data\n","y_pred = xgb_classifier.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SsflP2QQI570"},"outputs":[],"source":["from sklearn.metrics import classification_report, f1_score, accuracy_score,precision_score\n","f1=f1_score(y_test, y_pred, average='binary')\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","print(\"F1 score:\", f1)\n","print(\"Accuracy score:\", accuracy)\n","print(\"Precision:\", precision)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPq2IuLYNTuZjtGcHVDbFYy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}